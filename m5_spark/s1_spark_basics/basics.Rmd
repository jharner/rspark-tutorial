---
title: "Spark Basics"
author: "Jim Harner"
date: "1/12/2021"
output:
  html_document: default
  pdf_document: default
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The `sparklyr` package provides an R frontend to Spark based on a `dplyr` interface to Spark SQL. Once these required packages are loaded, a Spark connection is established.
```{r}
library(dplyr, warn.conflicts = FALSE)
library(sparklyr)

Sys.getenv("SPARK_HOME")
# start the sparklyr session locally or to the master container
# test \"/bin/spark-class/\" && echo 1 || echo 0
# if [ -d /bin/spark-class/ ]; then echo 1; else echo 0; fi
if(system("if [ -d /bin/spark-class/ ]; then echo 1; else echo 0; fi ") == 1){
  master <- "spark://master:7077"
} else{
  master <- "local"
}
sc <- spark_connect(master = master)
```
Spark's home is located in `/opt/spark` on Unbuntu 18.04.

[Spark](http://spark.apache.org/docs/latest/programming-guide.html#overview) is a general-purpose cluster computing system, which:

* has high-level APIs in Java, Scala, Python and R;  
* supports multi-step data pipelines structured as directed acyclic graphs (DAGs);
* supports in-memory data sharing across DAGs allowing different jobs to work with the same data.

We can use `bash` to determine the Spark version:
```{bash}
# echo $SPARK_HOME
# The following command displays the Spark version installed:
spark-submit --version 2>&1 | grep -v "SLF4J:"
```

Spark provides a unified framework to manage big data processing with a variety of data sets that are diverse in nature, e.g., text data, graph data, etc., as well as the source of data (batch vs. real-time streaming data).

Spark supports a rich set of higher-level tools including:   

* *Spark SQL* for running SQL-like queries on Spark data using the JDBC API or the Spark SQL CLI. Spark SQL allows users to extract data from different formats, (e.g., JSON, Parquet, or Hive), transform it, and load it for *ad-hoc* querying, i.e., ETL.  

* *MLlib* for machine learning, including classification, regression, clustering, collaborative filtering, dimensionality reduction, and the underlying optimization algorithms. MLlib uses the DataFrame API, which is built on the Spark SQL engine.

* *Structured Streaming* for real-time data processing. Spark streaming uses a fault-tolerant stream processing engine built on the Spark SQL engine. Thus, you can express your streaming computation the same way you would express a batch computation on static data. Using the DataFrame API, the Spark SQL engine will take care of running the analysis incrementally and it continuously update the final result as streaming data continues to arrive. 

The [SparkR](https://spark.apache.org/docs/latest/sparkr.html) package is another R frontend to Spark, which is officially supported as part of the Spark distribution. However, we will focus on the [sparklyr](https://spark.rstudio.com) since it ties into the larger tidyverse suite of packages.

## 5.1 Sparklyr Basics

The `sparklyr` package is being developed by RStudio. It is undergoing rapid expansion. See [RStudio's sparklyr](https://spark.rstudio.com) for information.

The `sparklyr` R package provides a `dplyr` backend to Spark. Using `sparklyr`, you can: 

* filter and aggregate Spark DataFrames and bring them into R for analysis and visualization;  
* develop workflows using `dplyr` and compatible R packages;  
* write R code to access Spark's machine learning library, [MLlib](http://spark.apache.org/docs/latest/mllib-guide.html);  
* create Spark extensions.  

Using `sparklyr`, connections can be made to local instances or to remote Spark clusters. In our default case the connection is to a local connection bundled in the `rstudio` container. Optionally, a master and 3 worker nodes are available in `rspark`.

The `sparklyr` library is loaded in the setup above and a Spark connection is established. The Spark connection `sc` provides the connector to Spark.

### 5.1.1 dplyr

The `dpyr` verbs, e.g., `mutate`, `filter`, can be used on Spark DataFrames. A more complete discussion is given in Section 5.2.

We will use the `flights` data in the `nycflights13` package as an example. If its size becomes an issue, execute each chunk in sequence in notebook mode.

```{r}
library(nycflights13)
str(flights)
```
The `flights` R data frame is a tibble, which allows large data to be displayed. This data frame has the date of departure, the actual departure time, etc. See the package documentation for variable definitions.

The `copy_to` function copies an R `data.frame` or `tibble` to Spark as a Spark SQL table. The resulting object is a `tbl_spark`, which is a `dplyr`-compatible interface to the Spark DataFrame.
```{r}
flights_tbl <- copy_to(sc, nycflights13::flights, "flights", overwrite = TRUE)
flights_tbl
src_tbls(sc)
```
By default, the `flights` Spark table is cached in memory (`memory = TRUE`), which speeds up computations, but by default the table is not partitioned (`repartition = 0L`) since we are not running an actual cluster by default. See the `copy_to` function in the `sparklyr` package for more details.

The Spark connection should be disconnected at the end of a task.
```{r}
spark_disconnect(sc)
```